{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For the social media challenges, we've harvested the tweets of about 60 NGOs and a number of hashtags. Additionally, 50 of those NGOs also have YouTube channels, where we harvested all channel and video comments.\n",
    "\n",
    "The results of these efforts have been stored as Hive tables. The following gives an overview of the tables that are available for the social media team:\n",
    "\n",
    "---------------------------------------------------------------------------------------\n",
    "Table                 |Contents\n",
    "----------------------|----------------------------------------------------------------\n",
    "`twitter`             |One row for each tweet, 73 variables describing the tweet.\n",
    "`twitter_translations`|One row for each tweet, sanitized and translated into English.\n",
    "`youtube_channels`    |One row for each channel comment in 15 variables.\n",
    "`youtube_videos`      |One row for each video in 16 variables.\n",
    "`youtube_comments`    |One row for each comment in 16 variables.\n",
    "`youtube_translations`|One row for each comment, sanitized and translated into English.\n",
    "`ngos`                |One row per targeted NGO\n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "You can find a more detailed description of the available fields in each table at the [Social Media data dictionary](https://github.com/MichalMilkowski1989/Datathon-Vienna-2018/wiki/Social-Media-Data-Dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data access\n",
    "\n",
    "All data is stored in tables in the `sm` Hive database. From within R, it's easiest to access the data\n",
    "using Spark's interface to Hive. Follow these steps to get a data set you can work with:\n",
    "\n",
    "1. Set up a Spark connection\n",
    "2. Write a SQL statement to retrieve the data you need\n",
    "3. Use that (subset of the) data directly in Spark\n",
    "4. Collect the data (or ideally, just some results) onto the Edge node.\n",
    "\n",
    "Find some code examples below for each of the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Spark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘SparkR’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var, window\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,\n",
      "    rank, rbind, sample, startsWith, subset, summary, transform, union\n",
      "\n",
      "Spark package found in SPARK_HOME: /usr/hdp/current/spark2-client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching java with spark-submit command /usr/hdp/current/spark2-client/bin/spark-submit   --driver-memory \"2g\" sparkr-shell /tmp/RtmpD4C16B/backend_portc51c3c81c376 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in sparkR.session(master = \"yarn\", appName = paste0(\"SparkR_\", Sys.getenv(\"USER\")), :\n",
      "“Version mismatch between Spark JVM and SparkR package. JVM version was 2.2.0.2.6.4.0-91 , while R package version was 2.2.0”"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.SparkSession id 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(SparkR)\n",
    "library(magrittr)\n",
    "\n",
    "sp_conf <- list(spark.driver.memory = \"2g\")\n",
    "\n",
    "sparkR.session(master = \"yarn\",\n",
    "               appName = paste0(\"SparkR_\", Sys.getenv(\"USER\")),\n",
    "               sparkConfig = sp_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering data (sub) sets as Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_channels_sdf <- sql(\"SELECT * FROM sm.youtube_channels\")\n",
    "twitter_sdf <- sql(\"SELECT * FROM sm.twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with the Spark DataFrame directly in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>lang</th><th scope=col>avg_rt</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>en       </td><td>2.7638003</td></tr>\n",
       "\t<tr><td>vi       </td><td>0.0000000</td></tr>\n",
       "\t<tr><td>ne       </td><td>1.6666667</td></tr>\n",
       "\t<tr><td>ps       </td><td>0.0000000</td></tr>\n",
       "\t<tr><td>ro       </td><td>1.0084388</td></tr>\n",
       "\t<tr><td>sl       </td><td>0.3630137</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " lang & avg\\_rt\\\\\n",
       "\\hline\n",
       "\t en        & 2.7638003\\\\\n",
       "\t vi        & 0.0000000\\\\\n",
       "\t ne        & 1.6666667\\\\\n",
       "\t ps        & 0.0000000\\\\\n",
       "\t ro        & 1.0084388\\\\\n",
       "\t sl        & 0.3630137\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "lang | avg_rt | \n",
       "|---|---|---|---|---|---|\n",
       "| en        | 2.7638003 | \n",
       "| vi        | 0.0000000 | \n",
       "| ne        | 1.6666667 | \n",
       "| ps        | 0.0000000 | \n",
       "| ro        | 1.0084388 | \n",
       "| sl        | 0.3630137 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  lang avg_rt   \n",
       "1 en   2.7638003\n",
       "2 vi   0.0000000\n",
       "3 ne   1.6666667\n",
       "4 ps   0.0000000\n",
       "5 ro   1.0084388\n",
       "6 sl   0.3630137"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "twitter_sdf %>% group_by(twitter_sdf$lang) %>% summarize(avg_rt = mean(twitter_sdf$retweet_count)) %>% head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Deviance Residuals: \n",
       "(Note: These are approximate quantiles with relative error <= 0.01)\n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-3.327  -2.563  -1.931  -0.904  72.987  \n",
       "\n",
       "Coefficients:\n",
       "                        Estimate  Std. Error   t value    Pr(>|t|)\n",
       "(Intercept)           1.8067e+00  4.5570e-03  396.4666  0.0000e+00\n",
       "quoted_retweet_count  3.6089e-06  5.5147e-07    6.5441  5.9844e-11\n",
       "\n",
       "(Dispersion parameter for poisson family taken to be 1)\n",
       "\n",
       "    Null deviance: 126663  on 8048  degrees of freedom\n",
       "Residual deviance: 126626  on 8047  degrees of freedom\n",
       "AIC: 149522\n",
       "\n",
       "Number of Fisher Scoring iterations: 6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_data_sdf <- twitter_sdf %>%\n",
    "  select(\"retweet_count\", \"quoted_retweet_count\") %>%\n",
    "  dropna() %>%\n",
    "  filter(.$retweet_count > 0) %>%\n",
    "  filter(.$quoted_retweet_count > 0)\n",
    "\n",
    "model <- spark.glm(model_data_sdf, retweet_count ~ quoted_retweet_count, family = \"poisson\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data/results to the edge node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>11</li>\n",
       "\t<li>15</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 11\n",
       "\\item 15\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 11\n",
       "2. 15\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 11 15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yt_chan_local <- youtube_channels_sdf %>% collect()\n",
    "dim(yt_chan_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkR.session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
